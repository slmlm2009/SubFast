# Story 6.1: Test Framework Setup & Structure

## Status
DONE
## Story
**As a** developer,  
**I want** a comprehensive testing framework with clear structure and conventions,  
**so that** I can write and execute tests for SubFast's functionality with confidence and consistency.

## Acceptance Criteria

1. Tests can be run individually or as a complete suite
2. Test output is clear and actionable with pass/fail indicators
3. Framework supports parameterized tests for pattern variations
4. No external dependencies required (stdlib `unittest` only)
5. Test discovery works automatically via `unittest`
6. Test structure is documented in README with examples
7. Basic test files exist for configuration and CSV reporting modules
8. **Comprehensive CSV test reports generated BY DEFAULT (always)**
9. **Reports include detailed statistics: total tests, passed, failed, skipped**
10. **Reports use bordered table format (like SubFast console output)**
11. **For pattern tests: pattern ID, pattern name, variations tested, specific failures**
12. **Reports are self-contained - everything needed to understand results**
13. **Reports clearly show what to fix and what works**
14. **Reports timestamped and saved to `tests/reports/` directory**

## Tasks / Subtasks

- [x] Task 1: Set up tests/ directory structure (AC: 6)
  - [x] Create `tests/` root directory if not exists
  - [x] Create `tests/fixtures/` subdirectory for test data
  - [x] Create `tests/1- Renaming/` subdirectory (already exists with pattern guide)
  - [x] Create `tests/2- Embedding/` subdirectory (already exists with samples)
  - [x] Verify integration test files present in embedding directory

- [x] Task 2: Create test runner and documentation (AC: 1, 2, 6)
  - [x] Create `tests/run_tests.py` - test execution script with options
  - [x] Add command line options: run all, run specific module, verbose output
  - [x] **Reports generated BY DEFAULT (no flag needed)**
  - [x] Create `tests/README.md` with:
    - Testing philosophy and approach
    - How to run tests (individual and suite)
    - Test naming conventions
    - How to add new tests
    - Example test structure
    - How to generate and view test reports
  - [x] Document test execution examples (single, module, full suite, with reports)

- [x] Task 3: Create base test utilities (AC: 3, 4)
  - [x] Create `tests/test_helpers.py` with:
    - Base test case classes
    - Common assertion helpers
    - File comparison utilities
    - Path handling helpers
    - Test data generation utilities (for future use)
  - [x] Import only stdlib modules (`unittest`, `pathlib`, `tempfile`, `shutil`)
  - [x] Add docstrings explaining each utility function

- [x] Task 4: Create starter test for config_loader module (AC: 7)
  - [x] Create `tests/test_config_loader.py`
  - [x] Test default configuration generation
  - [x] Test config file loading with valid config
  - [x] Test fallback behavior for missing config
  - [x] Test invalid boolean values fallback to defaults
  - [x] Verify all DEFAULT_CONFIG keys are present
  - [x] Use `unittest.TestCase` base class

- [x] Task 5: Create starter test for csv_reporter module (AC: 7)
  - [x] Create `tests/test_csv_reporter.py`
  - [x] Test CSV report structure (headers, sections)
  - [x] Test statistics calculation accuracy
  - [x] Test bordered text table formatting
  - [x] Test empty sections are omitted
  - [x] Use parameterized tests for different scenarios

- [x] Task 6: Create comprehensive CSV test report generator (AC: 8-14)
  - [x] Create `tests/test_reporter.py` module
  - [x] Implement `TestReporter` class that extends `unittest.TextTestResult`
  - [x] Capture detailed test results:
    - Test module, test class, test method
    - Status (PASS/FAIL/SKIP)
    - Duration (seconds)
    - Error message (full traceback if failed)
    - For pattern tests: pattern ID, pattern name, filename variation
  - [x] Generate **bordered table format** (like SubFast console):
    - Use dashes (---) and pipes (|) for borders
    - Headers with clear column names
    - Aligned columns for readability
  - [x] Include comprehensive statistics section:
    - Total Tests Run
    - Tests Passed (count and %)
    - Tests Failed (count and %)
    - Tests Skipped (count and %)
    - Total Execution Time
    - For pattern tests: Patterns tested, Pattern success rate
  - [x] Save to `tests/reports/test-results-YYYYMMDD-HHMMSS.txt` (text, not CSV)
  - [x] Create `tests/reports/` directory automatically
  - [x] Use Python stdlib only
  - [x] Format matches SubFast's bordered table style

- [x] Task 7: Integrate report generation with test runner (AC: 8, 14)
  - [x] **Generate report AUTOMATICALLY after every test run (no flag)**
  - [x] Print report to terminal AND save to file
  - [x] Terminal output shows:
    - Bordered statistics table
    - Summary of failures with pattern details
    - Report file location
  - [x] File contains complete detailed report
  - [x] Ensure report doesn't slow down test execution

- [x] Task 8: Verify test discovery and execution (AC: 1, 2)
  - [x] Run `python -m unittest discover tests/` successfully
  - [x] Verify all tests discovered and executed
  - [x] Check output clarity (test names, pass/fail, errors)
  - [x] Test individual file execution: `python -m unittest tests.test_config_loader`
  - [x] Verify execution time is fast (< 2 seconds for starter tests)
  - [x] Run with report generation: `python tests/run_tests.py`
  - [x] Verify report created in `tests/reports/`

- [x] Task 9: Document test structure in architecture (AC: 6)
  - [x] Documentation in `tests/README.md` completed
  - [x] Test directory structure documented
  - [x] Testing approach and conventions explained
  - [x] Report format and usage documented
  - [x] Story referenced as implementation guide

## Dev Notes

### Current System Context

**SubFast v3.2.0 Architecture:**
- **Shared Modules Location:** `subfast/scripts/common/`
  - `config_loader.py` - Configuration management (to be tested)
  - `pattern_engine.py` - Episode pattern matching (tested in Story 6.2)
  - `csv_reporter.py` - Report generation (to be tested)
- **Main Scripts:** `subfast/scripts/subfast_rename.py`, `subfast/scripts/subfast_embed.py`
- **Zero Dependencies:** Uses Python stdlib only

**Existing Test Resources:**
- `tests/1- Renaming/episode_patterns_guide.md` - Pattern specification (EXISTS)
- `tests/2- Embedding/integration_testing_files/` - Real video/subtitle samples (EXISTS)

### Testing Standards

**Framework:** Python's built-in `unittest` module (stdlib, zero dependencies)

**Key Principles from Coding Standards:**
1. **Path Handling:** Use `pathlib.Path` for all file operations
2. **Error Handling:** Tests should verify error handling behavior
3. **Atomic Operations:** Tests should clean up after themselves
4. **Subprocess Testing:** When testing mkvmerge (Story 6.3), use list-based arguments

**Test File Location:** `tests/` directory at project root

**Test Reports Location:** `tests/reports/` directory (auto-created)

**Test Standards:**
- Test file names: `test_<module_name>.py` (e.g., `test_config_loader.py`)
- Test class names: `Test<Functionality>` (e.g., `TestConfigLoading`)
- Test method names: `test_<specific_behavior>` (e.g., `test_default_config_generation`)
- Use `unittest.TestCase` as base class
- Use `setUp()` and `tearDown()` for test initialization/cleanup
- Use `self.assert*()` methods for assertions

**Test Organization:**
```python
import unittest
from pathlib import Path
from subfast.scripts.common import config_loader

class TestConfigLoader(unittest.TestCase):
    """Test configuration loading and generation."""
    
    def setUp(self):
        """Set up test fixtures before each test."""
        # Create temp directory, sample configs, etc.
        pass
    
    def test_default_config_generation(self):
        """Test that default configuration is generated correctly."""
        # Test implementation
        pass
    
    def tearDown(self):
        """Clean up after each test."""
        # Remove temp files, restore state, etc.
        pass

if __name__ == '__main__':
    unittest.main()
```

**Test Discovery:**
- Use `python -m unittest discover tests/` to run all tests
- Or `python -m unittest tests.test_config_loader` for specific module
- Or `python tests/run_tests.py` for custom runner

**Test Execution Speed Target:**
- Individual tests: < 1 second
- Full suite (this story only): < 2 seconds
- Future full suite (all stories): < 10 seconds

### Integration Points

**Modules to Test in This Story:**
1. **`subfast/scripts/common/config_loader.py`**
   - Functions: `load_config()`, `generate_default_config_file()`
   - Test config generation, loading, fallback behavior

2. **`subfast/scripts/common/csv_reporter.py`**
   - Functions: `generate_csv_report()`, `print_summary()`, statistics calculation
   - Test report structure, statistics accuracy, formatting

**Future Testing (Other Stories):**
- Story 6.2: Pattern matching with dummy files
- Story 6.3: Integration tests with real video/subtitle files
- Story 6.4: Test data management and extensibility

### Technical Notes

**test_helpers.py Utilities to Include:**
- `create_temp_directory()` - Creates temporary test directory
- `create_sample_config()` - Generates sample config.ini for testing
- `compare_files()` - Compares two files for equality
- `cleanup_test_files()` - Removes test artifacts
- `assert_file_exists()` - Custom assertion for file existence
- `assert_directory_empty()` - Checks directory is empty

**run_tests.py Features:**
- Default: Run all tests with `python run_tests.py` (report auto-generated)
- Module filter: `python run_tests.py test_config_loader`
- Verbose: `python run_tests.py -v`
- Help: `python run_tests.py --help`

**Report Generation:**
- **ALWAYS generated** (no flag needed)
- Printed to terminal with bordered tables
- Saved to timestamped file in `tests/reports/`

**Test Report Structure (Bordered Table Format):**
```
================================================================================
                          TEST EXECUTION REPORT                               
================================================================================
Test Run: 2025-01-16 10:30:15
Total Duration: 3.24 seconds

--------------------------------------------------------------------------------
                            SUMMARY STATISTICS                                 
--------------------------------------------------------------------------------
| Metric                    | Count  | Percentage |
|---------------------------|--------|------------|
| Total Tests Run           | 50     | 100.00%    |
| Tests Passed              | 47     | 94.00%     |
| Tests Failed              | 2      | 4.00%      |
| Tests Skipped             | 1      | 2.00%      |
| Total Execution Time      | 3.24s  | -          |
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
                        PATTERN MATCHING TESTS                                 
--------------------------------------------------------------------------------
| Pattern ID | Pattern Name     | Variations | Passed | Failed | Status |
|------------|------------------|------------|--------|--------|--------|
| 1          | S##E##           | 5          | 5      | 0      | PASS   |
| 2          | ##x##            | 4          | 4      | 0      | PASS   |
| 3          | S## - ##         | 3          | 2      | 1      | FAIL   |
| ...        | ...              | ...        | ...    | ...    | ...    |
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
                              FAILED TESTS                                      
--------------------------------------------------------------------------------
[FAIL] test_pattern_matching.TestPattern03.test_s_dash_variation_3
  Pattern: Pattern 3 (S## - ##)
  File: Show.S01-05.mkv
  Expected: S01E05
  Got: None
  Error: Pattern failed to match - check regex for dash spacing
  Duration: 0.001s

[FAIL] test_config_loader.TestConfigLoader.test_invalid_boolean
  Error: Boolean conversion failed for 'maybe' value
  Expected: Default fallback to 'true'
  Got: Exception raised
  Duration: 0.002s
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
                            DETAILED TEST RESULTS                               
--------------------------------------------------------------------------------
| Test Module         | Test Name                      | Status | Duration |
|---------------------|--------------------------------|--------|----------|
| test_config_loader  | test_default_config_generation | PASS   | 0.003s   |
| test_config_loader  | test_load_valid_config         | PASS   | 0.002s   |
| test_config_loader  | test_invalid_boolean           | FAIL   | 0.002s   |
| test_pattern_match  | test_pattern_01_var_1          | PASS   | 0.001s   |
| test_pattern_match  | test_pattern_01_var_2          | PASS   | 0.001s   |
| ...                 | ...                            | ...    | ...      |
--------------------------------------------------------------------------------

Report saved to: tests/reports/test-results-20250116-103015.txt
================================================================================
```

**Report Features:**
- **Bordered tables** with dashes and pipes (like SubFast console)
- **Summary statistics** at the top (total, passed, failed, skipped)
- **Pattern-specific section** showing which patterns passed/failed
- **Failed tests section** with detailed error information
- **Pattern details** for failures (pattern ID, name, filename, expected vs actual)
- **Actionable error messages** telling you exactly what to fix
- **Complete detailed results** at the bottom
- **Self-contained** - everything you need in one report
- **Timestamped filename**: `test-results-20250116-103015.txt`
- **Always generated** - no flag needed

### Success Criteria

- ✅ Test framework is operational
- ✅ Tests can be run easily (single command)
- ✅ Output is clear and actionable
- ✅ **Comprehensive test reports generated automatically**
- ✅ **Reports use bordered table format (like SubFast)**
- ✅ **Pattern-specific details included**
- ✅ **Summary statistics clear and actionable**
- ✅ **Reports saved to tests/reports/ directory**
- ✅ Foundation ready for Story 6.2 (pattern tests)
- ✅ Zero external dependencies maintained
- ✅ Documentation provides clear guidance

## Testing

**Test Validation for This Story:**
1. Run `python -m unittest discover tests/` → All tests pass
2. Run `python tests/run_tests.py` → Custom runner works
3. Run individual test file → Executes successfully
4. Check test output → Clear pass/fail indicators
5. Verify execution speed → < 2 seconds total
6. **Verify report auto-generation:**
   - Report printed to terminal with bordered tables
   - Report saved to `tests/reports/test-results-YYYYMMDD-HHMMSS.txt`
   - Summary statistics show total/passed/failed/skipped
7. **Check pattern test reporting:**
   - Pattern section shows all 25 patterns (when Story 6.2 done)
   - Each pattern shows variations tested
   - Failed patterns show which variation failed
   - Pattern ID and name clearly visible
8. **Review failed tests section:**
   - Each failure has pattern details (if pattern test)
   - Error messages are actionable
   - Expected vs actual values shown
   - Specific file that failed is named
9. **Open saved report file:**
   - Bordered table format matches terminal output
   - All sections present (summary, patterns, failures, detailed)
   - Report is self-contained and readable
10. **Run tests again:**
   - New report created (different timestamp)
   - Previous report preserved (historical tracking)
11. Check documentation → README explains report format

**Manual Verification:**
- Read `tests/README.md` → Report format and sections documented
- Review test file structure → Follows conventions
- Check imports → All stdlib, no external dependencies
- Verify cleanup → No leftover test artifacts after execution
- **Open test report:** → Bordered tables, clear statistics, pattern details
- **Check pattern section:** → Shows pattern ID, name, variations, status
- **Check failed section:** → Actionable errors with pattern context
- **Check tests/reports/ directory:** → Reports accumulate over time

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-16 | 1.0 | Initial story creation from Epic 6 | Scrum Master (Bob) |
| 2025-10-16 | 1.1 | Story implementation completed - all 9 tasks done | Dev Agent (James) |
| 2025-10-16 | 1.2 | Enhanced report: Added Test Script column and Test Script Summary section | Dev Agent (James) |

## Dev Agent Record

### Agent Model Used
Claude 3.5 Sonnet (Droid CLI)

### Debug Log References
- Test execution: 24 tests, 100% pass rate, 0.051s duration
- Report generated: `tests/reports/test-results-20251016-073802.txt`

### Completion Notes List
1. Created comprehensive test framework using Python stdlib unittest (zero dependencies)
2. Implemented bordered table reporting system matching SubFast console style
3. Reports generated AUTOMATICALLY by default (no flag required)
4. All 24 tests passing (14 config_loader + 10 csv_reporter)
5. Test execution speed: 0.050s (well under 2s target)
6. Report features: 
   - Overall summary statistics with percentages
   - **Test Script Summary** - Per-module statistics showing which modules are fully functioning
   - Detailed test results with **Test Script, Test Class, and Test Name** columns
   - Pattern breakdown section (ready for Story 6.2)
   - Failed tests section with detailed errors
7. User refinements applied:
   - Added Test Script column to detailed results for easy function identification
   - Added Test Script Summary section showing per-module health at a glance

### File List
**Created:**
- `tests/fixtures/` - Test data directory
- `tests/reports/` - Test reports directory  
- `tests/run_tests.py` - Custom test runner with comprehensive reporting
- `tests/README.md` - Complete testing documentation (philosophy, usage, examples)
- `tests/test_helpers.py` - Shared test utilities (temp dirs, assertions, file operations)
- `tests/test_config_loader.py` - Config loader tests (14 tests, all passing)
- `tests/test_csv_reporter.py` - CSV reporter tests (10 tests, all passing)
- `tests/test_reporter.py` - Comprehensive test report generator module

**Modified:**
- None (all new files)

**Deleted:**
- None

## QA Results
_To be populated by QA Agent after implementation review_
